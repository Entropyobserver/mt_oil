2025-12-04 10:01:35,323 - data_scaling - INFO - ================================================================================
2025-12-04 10:01:35,340 - data_scaling - INFO - EXPERIMENT 1: DATA SCALING ANALYSIS
2025-12-04 10:01:35,340 - data_scaling - INFO - ================================================================================
2025-12-04 10:01:35,341 - data_scaling - INFO - Using HF cache: /crex/proj/uppmax2025-3-5/private/yaxj1/hf_cache
2025-12-04 10:01:40,575 - data_scaling - INFO - Loading data splits
2025-12-04 10:01:40,942 - data_scaling - INFO - Train: 13622 samples
2025-12-04 10:01:40,942 - data_scaling - INFO - Val: 1702 samples
2025-12-04 10:01:40,943 - data_scaling - INFO - Test: 1704 samples
2025-12-04 10:01:40,943 - data_scaling - INFO - Data sizes to test: [100, 500, 1000, 2000, 4000, 6000, 8000, 10000, 13622]
2025-12-04 10:01:40,949 - data_scaling - INFO - 
============================================================
2025-12-04 10:01:40,954 - data_scaling - INFO - Experiment 1/9: Training with 100 samples
2025-12-04 10:01:40,959 - data_scaling - INFO - ============================================================
2025-12-04 10:01:45,914 - data_scaling - INFO - Starting training...
[2025-12-04 10:02:08,813][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
trainable params: 4,718,592 || all params: 1,144,502,272 || trainable%: 0.41228332310379195
[2025-12-04 10:02:19,383][accelerate.utils.other][WARNING] - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'train_runtime': 51.5647, 'train_samples_per_second': 5.818, 'train_steps_per_second': 0.349, 'train_loss': 2.6954589419894748, 'epoch': 2.88}
2025-12-04 10:20:23,740 - data_scaling - INFO - Generating predictions...
2025-12-04 10:32:24,677 - data_scaling - INFO - Results for size 100:
2025-12-04 10:32:24,705 - data_scaling - INFO -   Val  - BLEU: 0.1173, chrF: 29.15
2025-12-04 10:32:24,705 - data_scaling - INFO -   Test - BLEU: 0.1188, chrF: 29.56
2025-12-04 10:32:24,705 - data_scaling - INFO -   *** NEW BEST MODEL ***
2025-12-04 10:32:24,747 - data_scaling - INFO - 
============================================================
2025-12-04 10:32:24,747 - data_scaling - INFO - Experiment 2/9: Training with 500 samples
2025-12-04 10:32:24,747 - data_scaling - INFO - ============================================================
2025-12-04 10:32:31,028 - data_scaling - INFO - Starting training...
[2025-12-04 10:32:42,225][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
trainable params: 4,718,592 || all params: 1,144,502,272 || trainable%: 0.41228332310379195
[2025-12-04 10:32:52,944][accelerate.utils.other][WARNING] - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 2.2739, 'learning_rate': 0.00025, 'epoch': 1.6}
{'train_runtime': 224.5353, 'train_samples_per_second': 6.68, 'train_steps_per_second': 0.414, 'train_loss': 1.6738382975260417, 'epoch': 2.98}
2025-12-04 10:52:12,284 - data_scaling - INFO - Generating predictions...
2025-12-04 11:04:13,489 - data_scaling - INFO - Results for size 500:
2025-12-04 11:04:13,504 - data_scaling - INFO -   Val  - BLEU: 0.5536, chrF: 71.76
2025-12-04 11:04:13,504 - data_scaling - INFO -   Test - BLEU: 0.5653, chrF: 72.57
2025-12-04 11:04:13,505 - data_scaling - INFO -   *** NEW BEST MODEL ***
2025-12-04 11:04:13,526 - data_scaling - INFO - 
============================================================
2025-12-04 11:04:13,526 - data_scaling - INFO - Experiment 3/9: Training with 1000 samples
2025-12-04 11:04:13,526 - data_scaling - INFO - ============================================================
2025-12-04 11:04:17,857 - data_scaling - INFO - Starting training...
[2025-12-04 11:04:26,976][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
trainable params: 4,718,592 || all params: 1,144,502,272 || trainable%: 0.41228332310379195
[2025-12-04 11:04:36,292][accelerate.utils.other][WARNING] - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 2.2988, 'learning_rate': 0.00025, 'epoch': 0.8}
{'loss': 0.9858, 'learning_rate': 0.0005, 'epoch': 1.6}
{'loss': 0.7681, 'learning_rate': 0.00020930232558139536, 'epoch': 2.4}
{'train_runtime': 440.3618, 'train_samples_per_second': 6.813, 'train_steps_per_second': 0.422, 'train_loss': 1.2353982207595662, 'epoch': 2.98}
2025-12-04 11:27:28,553 - data_scaling - INFO - Generating predictions...
2025-12-04 11:39:45,506 - data_scaling - INFO - Results for size 1000:
2025-12-04 11:39:45,520 - data_scaling - INFO -   Val  - BLEU: 0.5972, chrF: 74.79
2025-12-04 11:39:45,521 - data_scaling - INFO -   Test - BLEU: 0.6108, chrF: 75.70
2025-12-04 11:39:45,521 - data_scaling - INFO -   *** NEW BEST MODEL ***
2025-12-04 11:39:45,546 - data_scaling - INFO - 
============================================================
2025-12-04 11:39:45,547 - data_scaling - INFO - Experiment 4/9: Training with 2000 samples
2025-12-04 11:39:45,547 - data_scaling - INFO - ============================================================
2025-12-04 11:39:51,394 - data_scaling - INFO - Starting training...
[2025-12-04 11:40:01,696][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
trainable params: 4,718,592 || all params: 1,144,502,272 || trainable%: 0.41228332310379195
[2025-12-04 11:40:12,024][accelerate.utils.other][WARNING] - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 2.3147, 'learning_rate': 0.00025, 'epoch': 0.4}
{'loss': 0.9948, 'learning_rate': 0.0005, 'epoch': 0.8}
{'loss': 0.7759, 'learning_rate': 0.00040909090909090913, 'epoch': 1.2}
{'loss': 0.7537, 'learning_rate': 0.0003181818181818182, 'epoch': 1.6}
{'eval_loss': 0.6504905819892883, 'eval_bleu': 0.6098418669850685, 'eval_chrf': 75.4830447522658, 'eval_runtime': 929.5048, 'eval_samples_per_second': 1.831, 'eval_steps_per_second': 0.458, 'epoch': 1.6}
{'loss': 0.6642, 'learning_rate': 0.00022727272727272727, 'epoch': 2.0}
{'loss': 0.6326, 'learning_rate': 0.00013636363636363637, 'epoch': 2.4}
{'loss': 0.6337, 'learning_rate': 4.545454545454546e-05, 'epoch': 2.8}
{'train_runtime': 1820.4086, 'train_samples_per_second': 3.296, 'train_steps_per_second': 0.206, 'train_loss': 0.9442327041625976, 'epoch': 3.0}
2025-12-04 12:26:06,479 - data_scaling - INFO - Generating predictions...
2025-12-04 12:37:56,798 - data_scaling - INFO - Results for size 2000:
2025-12-04 12:37:56,808 - data_scaling - INFO -   Val  - BLEU: 0.6243, chrF: 76.52
2025-12-04 12:37:56,808 - data_scaling - INFO -   Test - BLEU: 0.6325, chrF: 77.39
2025-12-04 12:37:56,808 - data_scaling - INFO -   *** NEW BEST MODEL ***
2025-12-04 12:37:56,818 - data_scaling - INFO - 
============================================================
2025-12-04 12:37:56,818 - data_scaling - INFO - Experiment 5/9: Training with 4000 samples
2025-12-04 12:37:56,819 - data_scaling - INFO - ============================================================
2025-12-04 12:38:02,247 - data_scaling - INFO - Starting training...
[2025-12-04 12:38:11,675][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
trainable params: 4,718,592 || all params: 1,144,502,272 || trainable%: 0.41228332310379195
[2025-12-04 12:38:21,796][accelerate.utils.other][WARNING] - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 2.3414, 'learning_rate': 0.00025, 'epoch': 0.2}
{'loss': 0.9803, 'learning_rate': 0.0005, 'epoch': 0.4}
{'loss': 0.7631, 'learning_rate': 0.0004615384615384616, 'epoch': 0.6}
{'loss': 0.6737, 'learning_rate': 0.0004230769230769231, 'epoch': 0.8}
{'eval_loss': 0.6526620388031006, 'eval_bleu': 0.6079802909673054, 'eval_chrf': 75.54232900655894, 'eval_runtime': 934.075, 'eval_samples_per_second': 1.822, 'eval_steps_per_second': 0.456, 'epoch': 0.8}
{'loss': 0.7103, 'learning_rate': 0.00038461538461538467, 'epoch': 1.0}
{'loss': 0.655, 'learning_rate': 0.00034615384615384613, 'epoch': 1.2}
{'loss': 0.5998, 'learning_rate': 0.0003076923076923077, 'epoch': 1.4}
{'loss': 0.6257, 'learning_rate': 0.0002692307692307692, 'epoch': 1.6}
{'eval_loss': 0.5946879982948303, 'eval_bleu': 0.6331176837527225, 'eval_chrf': 77.19623679414916, 'eval_runtime': 926.9782, 'eval_samples_per_second': 1.836, 'eval_steps_per_second': 0.46, 'epoch': 1.6}
{'loss': 0.6075, 'learning_rate': 0.0002307692307692308, 'epoch': 1.8}
{'loss': 0.6047, 'learning_rate': 0.00019230769230769233, 'epoch': 2.0}
{'loss': 0.5907, 'learning_rate': 0.00015384615384615385, 'epoch': 2.2}
{'loss': 0.5594, 'learning_rate': 0.0001153846153846154, 'epoch': 2.4}
{'eval_loss': 0.5814486742019653, 'eval_bleu': 0.6463271221114087, 'eval_chrf': 77.97536030244964, 'eval_runtime': 923.9544, 'eval_samples_per_second': 1.842, 'eval_steps_per_second': 0.461, 'epoch': 2.4}
{'loss': 0.559, 'learning_rate': 7.692307692307693e-05, 'epoch': 2.6}
{'loss': 0.5509, 'learning_rate': 3.846153846153846e-05, 'epoch': 2.8}
{'loss': 0.5435, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 4542.9918, 'train_samples_per_second': 2.641, 'train_steps_per_second': 0.165, 'train_loss': 0.757661875406901, 'epoch': 3.0}
2025-12-04 14:09:35,468 - data_scaling - INFO - Generating predictions...
2025-12-04 14:21:16,293 - data_scaling - INFO - Results for size 4000:
2025-12-04 14:21:16,302 - data_scaling - INFO -   Val  - BLEU: 0.6331, chrF: 77.20
2025-12-04 14:21:16,303 - data_scaling - INFO -   Test - BLEU: 0.6426, chrF: 78.00
2025-12-04 14:21:16,303 - data_scaling - INFO -   *** NEW BEST MODEL ***
2025-12-04 14:21:16,312 - data_scaling - INFO - 
============================================================
2025-12-04 14:21:16,312 - data_scaling - INFO - Experiment 6/9: Training with 6000 samples
2025-12-04 14:21:16,313 - data_scaling - INFO - ============================================================
2025-12-04 14:21:23,398 - data_scaling - INFO - Starting training...
[2025-12-04 14:21:33,116][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
trainable params: 4,718,592 || all params: 1,144,502,272 || trainable%: 0.41228332310379195
[2025-12-04 14:21:43,749][accelerate.utils.other][WARNING] - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 2.2432, 'learning_rate': 0.00025, 'epoch': 0.13}
{'loss': 0.9679, 'learning_rate': 0.0005, 'epoch': 0.27}
{'loss': 0.7792, 'learning_rate': 0.00047560975609756096, 'epoch': 0.4}
{'loss': 0.7484, 'learning_rate': 0.00045121951219512197, 'epoch': 0.53}
{'eval_loss': 0.6458683609962463, 'eval_bleu': 0.6106562822963034, 'eval_chrf': 75.71444214760022, 'eval_runtime': 938.9056, 'eval_samples_per_second': 1.813, 'eval_steps_per_second': 0.454, 'epoch': 0.53}
{'loss': 0.647, 'learning_rate': 0.0004273170731707317, 'epoch': 0.67}
{'loss': 0.663, 'learning_rate': 0.0004029268292682927, 'epoch': 0.8}
{'loss': 0.6373, 'learning_rate': 0.00037853658536585367, 'epoch': 0.93}
{'loss': 0.6125, 'learning_rate': 0.0003541463414634146, 'epoch': 1.07}
{'eval_loss': 0.5939238667488098, 'eval_bleu': 0.6373988038411585, 'eval_chrf': 77.46978226182458, 'eval_runtime': 926.489, 'eval_samples_per_second': 1.837, 'eval_steps_per_second': 0.46, 'epoch': 1.07}
{'loss': 0.6342, 'learning_rate': 0.00032975609756097563, 'epoch': 1.2}
{'loss': 0.5752, 'learning_rate': 0.0003053658536585366, 'epoch': 1.33}
{'loss': 0.5895, 'learning_rate': 0.0002809756097560976, 'epoch': 1.47}
{'loss': 0.5512, 'learning_rate': 0.00025658536585365855, 'epoch': 1.6}
{'eval_loss': 0.5738492608070374, 'eval_bleu': 0.6482356291883978, 'eval_chrf': 78.09878391729414, 'eval_runtime': 933.0065, 'eval_samples_per_second': 1.824, 'eval_steps_per_second': 0.457, 'epoch': 1.6}
{'loss': 0.5728, 'learning_rate': 0.00023219512195121953, 'epoch': 1.73}
{'loss': 0.5483, 'learning_rate': 0.00020780487804878048, 'epoch': 1.87}
{'loss': 0.6088, 'learning_rate': 0.00018341463414634147, 'epoch': 2.0}
{'loss': 0.5291, 'learning_rate': 0.00015902439024390245, 'epoch': 2.13}
{'eval_loss': 0.5572273135185242, 'eval_bleu': 0.6529024319937774, 'eval_chrf': 78.33550925074667, 'eval_runtime': 934.967, 'eval_samples_per_second': 1.82, 'eval_steps_per_second': 0.456, 'epoch': 2.13}
{'loss': 0.5522, 'learning_rate': 0.00013463414634146343, 'epoch': 2.27}
{'loss': 0.5238, 'learning_rate': 0.0001102439024390244, 'epoch': 2.4}
{'loss': 0.526, 'learning_rate': 8.585365853658538e-05, 'epoch': 2.53}
{'loss': 0.5202, 'learning_rate': 6.146341463414634e-05, 'epoch': 2.67}
{'eval_loss': 0.5506778955459595, 'eval_bleu': 0.6567953061809221, 'eval_chrf': 78.72201864475252, 'eval_runtime': 934.31, 'eval_samples_per_second': 1.822, 'eval_steps_per_second': 0.456, 'epoch': 2.67}
{'loss': 0.5221, 'learning_rate': 3.707317073170732e-05, 'epoch': 2.8}
{'loss': 0.548, 'learning_rate': 1.2682926829268294e-05, 'epoch': 2.93}
{'train_runtime': 7303.7522, 'train_samples_per_second': 2.464, 'train_steps_per_second': 0.154, 'train_loss': 0.6822342859903971, 'epoch': 3.0}
2025-12-04 16:38:59,837 - data_scaling - INFO - Generating predictions...
2025-12-04 16:51:09,231 - data_scaling - INFO - Results for size 6000:
2025-12-04 16:51:09,242 - data_scaling - INFO -   Val  - BLEU: 0.6529, chrF: 78.34
2025-12-04 16:51:09,243 - data_scaling - INFO -   Test - BLEU: 0.6588, chrF: 78.96
2025-12-04 16:51:09,243 - data_scaling - INFO -   *** NEW BEST MODEL ***
2025-12-04 16:51:09,253 - data_scaling - INFO - 
============================================================
2025-12-04 16:51:09,253 - data_scaling - INFO - Experiment 7/9: Training with 8000 samples
2025-12-04 16:51:09,253 - data_scaling - INFO - ============================================================
2025-12-04 16:51:15,086 - data_scaling - INFO - Starting training...
[2025-12-04 16:51:24,580][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
trainable params: 4,718,592 || all params: 1,144,502,272 || trainable%: 0.41228332310379195
[2025-12-04 16:51:35,571][accelerate.utils.other][WARNING] - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 2.2501, 'learning_rate': 0.00025, 'epoch': 0.1}
{'loss': 0.9609, 'learning_rate': 0.0005, 'epoch': 0.2}
{'loss': 0.7833, 'learning_rate': 0.00048214285714285715, 'epoch': 0.3}
{'loss': 0.7315, 'learning_rate': 0.00046428571428571433, 'epoch': 0.4}
{'eval_loss': 0.6473243236541748, 'eval_bleu': 0.6119184201671265, 'eval_chrf': 75.9491847883359, 'eval_runtime': 965.2027, 'eval_samples_per_second': 1.763, 'eval_steps_per_second': 0.441, 'epoch': 0.4}
{'loss': 0.6543, 'learning_rate': 0.00044642857142857147, 'epoch': 0.5}
{'loss': 0.6579, 'learning_rate': 0.00042857142857142855, 'epoch': 0.6}
{'loss': 0.6238, 'learning_rate': 0.0004107142857142857, 'epoch': 0.7}
{'loss': 0.6531, 'learning_rate': 0.0003928571428571429, 'epoch': 0.8}
{'eval_loss': 0.5901345014572144, 'eval_bleu': 0.6370433666739053, 'eval_chrf': 77.38741306762014, 'eval_runtime': 925.3091, 'eval_samples_per_second': 1.839, 'eval_steps_per_second': 0.46, 'epoch': 0.8}
{'loss': 0.6705, 'learning_rate': 0.000375, 'epoch': 0.9}
{'loss': 0.5861, 'learning_rate': 0.00035714285714285714, 'epoch': 1.0}
{'loss': 0.5607, 'learning_rate': 0.00033928571428571433, 'epoch': 1.1}
{'loss': 0.584, 'learning_rate': 0.00032142857142857147, 'epoch': 1.2}
{'eval_loss': 0.5670096278190613, 'eval_bleu': 0.6465305728246766, 'eval_chrf': 78.1638645424022, 'eval_runtime': 922.0568, 'eval_samples_per_second': 1.846, 'eval_steps_per_second': 0.462, 'epoch': 1.2}
{'loss': 0.5758, 'learning_rate': 0.00030357142857142855, 'epoch': 1.3}
{'loss': 0.5671, 'learning_rate': 0.0002857142857142857, 'epoch': 1.4}
{'loss': 0.5352, 'learning_rate': 0.00026785714285714287, 'epoch': 1.5}
{'loss': 0.5147, 'learning_rate': 0.00025, 'epoch': 1.6}
{'eval_loss': 0.5551479458808899, 'eval_bleu': 0.6553259374868109, 'eval_chrf': 78.47675980581181, 'eval_runtime': 929.0758, 'eval_samples_per_second': 1.832, 'eval_steps_per_second': 0.459, 'epoch': 1.6}
{'loss': 0.5936, 'learning_rate': 0.00023214285714285717, 'epoch': 1.7}
{'loss': 0.53, 'learning_rate': 0.00021428571428571427, 'epoch': 1.8}
{'loss': 0.5666, 'learning_rate': 0.00019642857142857144, 'epoch': 1.9}
{'loss': 0.5724, 'learning_rate': 0.00017857142857142857, 'epoch': 2.0}
{'eval_loss': 0.541538417339325, 'eval_bleu': 0.6611580776035375, 'eval_chrf': 78.94342424026445, 'eval_runtime': 927.0903, 'eval_samples_per_second': 1.836, 'eval_steps_per_second': 0.46, 'epoch': 2.0}
{'loss': 0.4976, 'learning_rate': 0.00016107142857142855, 'epoch': 2.1}
{'loss': 0.5068, 'learning_rate': 0.00014321428571428571, 'epoch': 2.2}
{'loss': 0.4974, 'learning_rate': 0.00012535714285714288, 'epoch': 2.3}
{'loss': 0.5035, 'learning_rate': 0.0001075, 'epoch': 2.4}
{'eval_loss': 0.539192795753479, 'eval_bleu': 0.6611051868360472, 'eval_chrf': 78.86784459327562, 'eval_runtime': 923.8638, 'eval_samples_per_second': 1.842, 'eval_steps_per_second': 0.461, 'epoch': 2.4}
{'loss': 0.5402, 'learning_rate': 8.964285714285715e-05, 'epoch': 2.5}
{'loss': 0.5201, 'learning_rate': 7.17857142857143e-05, 'epoch': 2.6}
{'loss': 0.516, 'learning_rate': 5.392857142857143e-05, 'epoch': 2.7}
{'loss': 0.5443, 'learning_rate': 3.607142857142858e-05, 'epoch': 2.8}
{'eval_loss': 0.535371720790863, 'eval_bleu': 0.6623468457877516, 'eval_chrf': 78.96906831614645, 'eval_runtime': 927.9957, 'eval_samples_per_second': 1.834, 'eval_steps_per_second': 0.459, 'epoch': 2.8}
{'loss': 0.509, 'learning_rate': 1.8214285714285715e-05, 'epoch': 2.9}
{'loss': 0.5178, 'learning_rate': 3.5714285714285716e-07, 'epoch': 3.0}
{'train_runtime': 10014.699, 'train_samples_per_second': 2.396, 'train_steps_per_second': 0.15, 'train_loss': 0.6441447766621907, 'epoch': 3.0}
2025-12-04 19:54:07,664 - data_scaling - INFO - Generating predictions...
2025-12-04 20:06:03,039 - data_scaling - INFO - Results for size 8000:
2025-12-04 20:06:03,053 - data_scaling - INFO -   Val  - BLEU: 0.6611, chrF: 78.87
2025-12-04 20:06:03,054 - data_scaling - INFO -   Test - BLEU: 0.6696, chrF: 79.73
2025-12-04 20:06:03,054 - data_scaling - INFO -   *** NEW BEST MODEL ***
2025-12-04 20:06:03,067 - data_scaling - INFO - 
============================================================
2025-12-04 20:06:03,067 - data_scaling - INFO - Experiment 8/9: Training with 10000 samples
2025-12-04 20:06:03,067 - data_scaling - INFO - ============================================================
2025-12-04 20:06:09,718 - data_scaling - INFO - Starting training...
[2025-12-04 20:06:19,195][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
trainable params: 4,718,592 || all params: 1,144,502,272 || trainable%: 0.41228332310379195
[2025-12-04 20:06:31,552][accelerate.utils.other][WARNING] - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 2.2861, 'learning_rate': 0.00025, 'epoch': 0.08}
{'loss': 0.8933, 'learning_rate': 0.0005, 'epoch': 0.16}
{'loss': 0.7739, 'learning_rate': 0.0004859154929577465, 'epoch': 0.24}
{'loss': 0.7456, 'learning_rate': 0.0004718309859154929, 'epoch': 0.32}
{'eval_loss': 0.645973265171051, 'eval_bleu': 0.6154443745264543, 'eval_chrf': 76.10723074731345, 'eval_runtime': 927.8438, 'eval_samples_per_second': 1.834, 'eval_steps_per_second': 0.459, 'epoch': 0.32}
{'loss': 0.6669, 'learning_rate': 0.00045774647887323943, 'epoch': 0.4}
{'loss': 0.7059, 'learning_rate': 0.00044366197183098594, 'epoch': 0.48}
{'loss': 0.6356, 'learning_rate': 0.0004295774647887324, 'epoch': 0.56}
{'loss': 0.6333, 'learning_rate': 0.00041549295774647886, 'epoch': 0.64}
{'eval_loss': 0.5914744734764099, 'eval_bleu': 0.634954410161835, 'eval_chrf': 77.35563784581217, 'eval_runtime': 923.654, 'eval_samples_per_second': 1.843, 'eval_steps_per_second': 0.461, 'epoch': 0.64}
{'loss': 0.592, 'learning_rate': 0.00040140845070422537, 'epoch': 0.72}
{'loss': 0.6303, 'learning_rate': 0.0003873239436619718, 'epoch': 0.8}
{'loss': 0.5697, 'learning_rate': 0.00037323943661971834, 'epoch': 0.88}
{'loss': 0.6039, 'learning_rate': 0.0003591549295774648, 'epoch': 0.96}
{'eval_loss': 0.5634879469871521, 'eval_bleu': 0.6493522348180125, 'eval_chrf': 78.15070034239756, 'eval_runtime': 924.938, 'eval_samples_per_second': 1.84, 'eval_steps_per_second': 0.461, 'epoch': 0.96}
{'loss': 0.5605, 'learning_rate': 0.00034507042253521125, 'epoch': 1.04}
{'loss': 0.5189, 'learning_rate': 0.00033098591549295776, 'epoch': 1.12}
{'loss': 0.5806, 'learning_rate': 0.00031690140845070427, 'epoch': 1.2}
{'loss': 0.5644, 'learning_rate': 0.00030281690140845067, 'epoch': 1.28}
{'eval_loss': 0.548781156539917, 'eval_bleu': 0.6545601871632649, 'eval_chrf': 78.31211768153659, 'eval_runtime': 918.9053, 'eval_samples_per_second': 1.852, 'eval_steps_per_second': 0.464, 'epoch': 1.28}
{'loss': 0.5225, 'learning_rate': 0.0002887323943661972, 'epoch': 1.36}
{'loss': 0.5931, 'learning_rate': 0.0002746478873239437, 'epoch': 1.44}
{'loss': 0.5645, 'learning_rate': 0.00026056338028169015, 'epoch': 1.52}
{'loss': 0.5502, 'learning_rate': 0.0002464788732394366, 'epoch': 1.6}
{'eval_loss': 0.5384462475776672, 'eval_bleu': 0.6577517947876084, 'eval_chrf': 78.84574519005169, 'eval_runtime': 924.7269, 'eval_samples_per_second': 1.841, 'eval_steps_per_second': 0.461, 'epoch': 1.6}
{'loss': 0.5606, 'learning_rate': 0.0002323943661971831, 'epoch': 1.68}
{'loss': 0.5336, 'learning_rate': 0.00021830985915492957, 'epoch': 1.76}
{'loss': 0.5221, 'learning_rate': 0.00020422535211267606, 'epoch': 1.84}
{'loss': 0.5076, 'learning_rate': 0.00019014084507042254, 'epoch': 1.92}
{'eval_loss': 0.5317591428756714, 'eval_bleu': 0.6589531434799959, 'eval_chrf': 78.79810445198191, 'eval_runtime': 930.0051, 'eval_samples_per_second': 1.83, 'eval_steps_per_second': 0.458, 'epoch': 1.92}
{'loss': 0.5068, 'learning_rate': 0.000176056338028169, 'epoch': 2.0}
{'loss': 0.4802, 'learning_rate': 0.0001619718309859155, 'epoch': 2.08}
{'loss': 0.4642, 'learning_rate': 0.00014788732394366196, 'epoch': 2.16}
{'loss': 0.5154, 'learning_rate': 0.00013380281690140845, 'epoch': 2.24}
{'eval_loss': 0.5270738005638123, 'eval_bleu': 0.6650476948159576, 'eval_chrf': 79.20402952501203, 'eval_runtime': 938.2842, 'eval_samples_per_second': 1.814, 'eval_steps_per_second': 0.454, 'epoch': 2.24}
{'loss': 0.5224, 'learning_rate': 0.00011971830985915493, 'epoch': 2.32}
{'loss': 0.4797, 'learning_rate': 0.00010563380281690141, 'epoch': 2.4}
{'loss': 0.5162, 'learning_rate': 9.15492957746479e-05, 'epoch': 2.48}
{'loss': 0.4645, 'learning_rate': 7.746478873239437e-05, 'epoch': 2.56}
{'eval_loss': 0.5255900025367737, 'eval_bleu': 0.6671302925051847, 'eval_chrf': 79.4307586117884, 'eval_runtime': 926.3178, 'eval_samples_per_second': 1.837, 'eval_steps_per_second': 0.46, 'epoch': 2.56}
{'loss': 0.5189, 'learning_rate': 6.338028169014085e-05, 'epoch': 2.64}
{'loss': 0.4888, 'learning_rate': 4.929577464788732e-05, 'epoch': 2.72}
{'loss': 0.5126, 'learning_rate': 3.5211267605633805e-05, 'epoch': 2.8}
{'loss': 0.5434, 'learning_rate': 2.112676056338028e-05, 'epoch': 2.88}
{'eval_loss': 0.5220937132835388, 'eval_bleu': 0.6686427340092649, 'eval_chrf': 79.44886481512124, 'eval_runtime': 932.4742, 'eval_samples_per_second': 1.825, 'eval_steps_per_second': 0.457, 'epoch': 2.88}
{'loss': 0.4572, 'learning_rate': 7.042253521126761e-06, 'epoch': 2.96}
{'train_runtime': 12746.1692, 'train_samples_per_second': 2.354, 'train_steps_per_second': 0.147, 'train_loss': 0.6143523300170899, 'epoch': 3.0}
2025-12-04 23:54:40,258 - data_scaling - INFO - Generating predictions...
2025-12-05 00:06:37,828 - data_scaling - INFO - Results for size 10000:
2025-12-05 00:06:37,840 - data_scaling - INFO -   Val  - BLEU: 0.6671, chrF: 79.43
2025-12-05 00:06:37,840 - data_scaling - INFO -   Test - BLEU: 0.6720, chrF: 79.95
2025-12-05 00:06:37,841 - data_scaling - INFO -   *** NEW BEST MODEL ***
2025-12-05 00:06:37,851 - data_scaling - INFO - 
============================================================
2025-12-05 00:06:37,851 - data_scaling - INFO - Experiment 9/9: Training with 13622 samples
2025-12-05 00:06:37,851 - data_scaling - INFO - ============================================================
2025-12-05 00:06:44,161 - data_scaling - INFO - Starting training...
[2025-12-05 00:06:53,889][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
trainable params: 4,718,592 || all params: 1,144,502,272 || trainable%: 0.41228332310379195
[2025-12-05 00:07:06,578][accelerate.utils.other][WARNING] - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 2.3019, 'learning_rate': 0.00025, 'epoch': 0.06}
{'loss': 0.985, 'learning_rate': 0.0005, 'epoch': 0.12}
{'loss': 0.7484, 'learning_rate': 0.0004898083978801468, 'epoch': 0.18}
{'loss': 0.6728, 'learning_rate': 0.0004796167957602935, 'epoch': 0.23}
{'eval_loss': 0.6499167680740356, 'eval_bleu': 0.6106431121615963, 'eval_chrf': 75.8280156934913, 'eval_runtime': 931.6378, 'eval_samples_per_second': 1.827, 'eval_steps_per_second': 0.457, 'epoch': 0.23}
{'loss': 0.691, 'learning_rate': 0.0004694251936404403, 'epoch': 0.29}
{'loss': 0.647, 'learning_rate': 0.00045923359152058704, 'epoch': 0.35}
{'loss': 0.6362, 'learning_rate': 0.0004490419894007338, 'epoch': 0.41}
{'loss': 0.6743, 'learning_rate': 0.00043885038728088056, 'epoch': 0.47}
{'eval_loss': 0.5899657607078552, 'eval_bleu': 0.6333512055333324, 'eval_chrf': 77.26617846886816, 'eval_runtime': 925.9028, 'eval_samples_per_second': 1.838, 'eval_steps_per_second': 0.46, 'epoch': 0.47}
{'loss': 0.6252, 'learning_rate': 0.0004286587851610273, 'epoch': 0.53}
{'loss': 0.5736, 'learning_rate': 0.00041846718304117407, 'epoch': 0.59}
{'loss': 0.615, 'learning_rate': 0.00040827558092132086, 'epoch': 0.65}
{'loss': 0.6257, 'learning_rate': 0.0003980839788014676, 'epoch': 0.7}
{'eval_loss': 0.559955894947052, 'eval_bleu': 0.6559092654603319, 'eval_chrf': 78.5100262370902, 'eval_runtime': 930.7082, 'eval_samples_per_second': 1.829, 'eval_steps_per_second': 0.458, 'epoch': 0.7}
{'loss': 0.6271, 'learning_rate': 0.00038789237668161437, 'epoch': 0.76}
{'loss': 0.5955, 'learning_rate': 0.0003777007745617611, 'epoch': 0.82}
{'loss': 0.5538, 'learning_rate': 0.0003675091724419079, 'epoch': 0.88}
{'loss': 0.5674, 'learning_rate': 0.0003573175703220546, 'epoch': 0.94}
{'eval_loss': 0.549271821975708, 'eval_bleu': 0.6565093580038811, 'eval_chrf': 78.58195450979764, 'eval_runtime': 926.8236, 'eval_samples_per_second': 1.836, 'eval_steps_per_second': 0.46, 'epoch': 0.94}
{'loss': 0.585, 'learning_rate': 0.0003471259682022014, 'epoch': 1.0}
{'loss': 0.5269, 'learning_rate': 0.0003369343660823482, 'epoch': 1.06}
{'loss': 0.526, 'learning_rate': 0.0003267427639624949, 'epoch': 1.12}
{'loss': 0.5061, 'learning_rate': 0.00031655116184264165, 'epoch': 1.17}
{'eval_loss': 0.5420472621917725, 'eval_bleu': 0.6611553335456635, 'eval_chrf': 79.10401777814566, 'eval_runtime': 925.0399, 'eval_samples_per_second': 1.84, 'eval_steps_per_second': 0.461, 'epoch': 1.17}
{'loss': 0.525, 'learning_rate': 0.00030635955972278843, 'epoch': 1.23}
{'loss': 0.5685, 'learning_rate': 0.00029616795760293517, 'epoch': 1.29}
{'loss': 0.5115, 'learning_rate': 0.00028597635548308195, 'epoch': 1.35}
{'loss': 0.542, 'learning_rate': 0.00027578475336322873, 'epoch': 1.41}
{'eval_loss': 0.5288988947868347, 'eval_bleu': 0.6623716186377491, 'eval_chrf': 79.05337776774735, 'eval_runtime': 926.068, 'eval_samples_per_second': 1.838, 'eval_steps_per_second': 0.46, 'epoch': 1.41}
{'loss': 0.5118, 'learning_rate': 0.00026559315124337547, 'epoch': 1.47}
{'loss': 0.5435, 'learning_rate': 0.0002554015491235222, 'epoch': 1.53}
{'loss': 0.501, 'learning_rate': 0.000245209947003669, 'epoch': 1.59}
{'loss': 0.5644, 'learning_rate': 0.00023501834488381574, 'epoch': 1.64}
{'eval_loss': 0.5228614211082458, 'eval_bleu': 0.6652561033747362, 'eval_chrf': 79.10347432796586, 'eval_runtime': 929.7125, 'eval_samples_per_second': 1.831, 'eval_steps_per_second': 0.458, 'epoch': 1.64}
{'loss': 0.5359, 'learning_rate': 0.0002248267427639625, 'epoch': 1.7}
{'loss': 0.4869, 'learning_rate': 0.00021463514064410925, 'epoch': 1.76}
{'loss': 0.5074, 'learning_rate': 0.000204443538524256, 'epoch': 1.82}
{'loss': 0.5218, 'learning_rate': 0.00019425193640440277, 'epoch': 1.88}
{'eval_loss': 0.5201687216758728, 'eval_bleu': 0.6672735018993226, 'eval_chrf': 79.4364518223792, 'eval_runtime': 934.4866, 'eval_samples_per_second': 1.821, 'eval_steps_per_second': 0.456, 'epoch': 1.88}
{'loss': 0.5363, 'learning_rate': 0.00018406033428454955, 'epoch': 1.94}
{'loss': 0.5157, 'learning_rate': 0.00017386873216469629, 'epoch': 2.0}
{'loss': 0.5032, 'learning_rate': 0.00016367713004484304, 'epoch': 2.06}
{'loss': 0.5057, 'learning_rate': 0.00015348552792498983, 'epoch': 2.11}
{'eval_loss': 0.5145429372787476, 'eval_bleu': 0.6699066500176165, 'eval_chrf': 79.58407681495385, 'eval_runtime': 933.7338, 'eval_samples_per_second': 1.823, 'eval_steps_per_second': 0.456, 'epoch': 2.11}
{'loss': 0.4893, 'learning_rate': 0.00014329392580513656, 'epoch': 2.17}
{'loss': 0.4827, 'learning_rate': 0.00013310232368528332, 'epoch': 2.23}
{'loss': 0.504, 'learning_rate': 0.0001229107215654301, 'epoch': 2.29}
{'loss': 0.4844, 'learning_rate': 0.00011271911944557685, 'epoch': 2.35}
{'eval_loss': 0.5137790441513062, 'eval_bleu': 0.6733887309817849, 'eval_chrf': 79.78812581491992, 'eval_runtime': 929.1676, 'eval_samples_per_second': 1.832, 'eval_steps_per_second': 0.458, 'epoch': 2.35}
{'loss': 0.5024, 'learning_rate': 0.0001025275173257236, 'epoch': 2.41}
{'loss': 0.4877, 'learning_rate': 9.233591520587037e-05, 'epoch': 2.47}
{'loss': 0.4539, 'learning_rate': 8.214431308601712e-05, 'epoch': 2.52}
{'loss': 0.5146, 'learning_rate': 7.195271096616388e-05, 'epoch': 2.58}
{'eval_loss': 0.5110887289047241, 'eval_bleu': 0.6753913973340397, 'eval_chrf': 79.91659401079602, 'eval_runtime': 941.9364, 'eval_samples_per_second': 1.807, 'eval_steps_per_second': 0.452, 'epoch': 2.58}
{'loss': 0.5048, 'learning_rate': 6.176110884631065e-05, 'epoch': 2.64}
{'loss': 0.4615, 'learning_rate': 5.1569506726457405e-05, 'epoch': 2.7}
{'loss': 0.508, 'learning_rate': 4.1377904606604156e-05, 'epoch': 2.76}
{'loss': 0.4819, 'learning_rate': 3.1186302486750914e-05, 'epoch': 2.82}
{'eval_loss': 0.5086098909378052, 'eval_bleu': 0.6754785179297017, 'eval_chrf': 79.91766318137299, 'eval_runtime': 935.5868, 'eval_samples_per_second': 1.819, 'eval_steps_per_second': 0.455, 'epoch': 2.82}
{'loss': 0.4265, 'learning_rate': 2.099470036689768e-05, 'epoch': 2.88}
{'loss': 0.5088, 'learning_rate': 1.0803098247044436e-05, 'epoch': 2.94}
{'loss': 0.4478, 'learning_rate': 6.114961271911944e-07, 'epoch': 2.99}
{'train_runtime': 17157.1537, 'train_samples_per_second': 2.382, 'train_steps_per_second': 0.149, 'train_loss': 0.5866296105976904, 'epoch': 3.0}
2025-12-05 05:08:41,106 - data_scaling - INFO - Generating predictions...
2025-12-05 05:20:28,419 - data_scaling - INFO - Results for size 13622:
2025-12-05 05:20:28,432 - data_scaling - INFO -   Val  - BLEU: 0.6755, chrF: 79.92
2025-12-05 05:20:28,432 - data_scaling - INFO -   Test - BLEU: 0.6812, chrF: 80.40
2025-12-05 05:20:28,433 - data_scaling - INFO -   *** NEW BEST MODEL ***
2025-12-05 05:20:28,445 - data_scaling - INFO - 
================================================================================
2025-12-05 05:20:28,445 - data_scaling - INFO - DATA SCALING ANALYSIS COMPLETE
2025-12-05 05:20:28,445 - data_scaling - INFO - ================================================================================
2025-12-05 05:20:29,531 - data_scaling - INFO - Results saved to 40_outputs/01_data_scaling/results.csv
