num_train_epochs: 3

batch_size:
  train: 4
  eval: 4

gradient_accumulation_steps: 4

optimizer:
  name: adamw
  learning_rate: 5.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

lr_scheduler:
  type: linear
  warmup_steps: 100

evaluation:
  strategy: steps
  eval_steps: 200
  metric_for_best_model: bleu
  greater_is_better: true

save:
  strategy: steps
  save_steps: 400
  save_total_limit: 2
  load_best_model_at_end: true

early_stopping:
  patience: 3
  threshold: 0.001

generation:
  max_length: 128
  num_beams: 5
  predict_with_generate: true

compute:
  fp16: true
  max_grad_norm: 1.0
  dataloader_num_workers: 0
  dataloader_pin_memory: false

logging:
  logging_steps: 50
  report_to: []