# 00_configs/02_adapter/01_lora_r8.yaml
method: lora
task_type: SEQ_2_SEQ_LM
r: 8
lora_alpha: 16
lora_dropout: 0.1
target_modules:
  - q_proj
  - v_proj
  - k_proj
  - out_proj
bias: none
fan_in_fan_out: false
trainable_params: ~1M
trainable_percentage: ~0.17%