# 00_configs/02_adapter/02_lora_r16.yaml
method: lora
task_type: SEQ_2_SEQ_LM
r: 16
lora_alpha: 32
lora_dropout: 0.1
target_modules:
  - q_proj
  - v_proj
  - k_proj
  - out_proj
bias: none
fan_in_fan_out: false
trainable_params: ~2M
trainable_percentage: ~0.33%