# 00_configs/02_adapter/03_lora_r32.yaml 
method: lora
task_type: SEQ_2_SEQ_LM
r: 32
lora_alpha: 64
lora_dropout: 0.1
target_modules:
  - q_proj
  - v_proj
  - k_proj
  - out_proj
bias: none
fan_in_fan_out: false
trainable_params: ~4M
trainable_percentage: ~0.67%