{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c18f8948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING IN FULL MODE\n",
      "Initializing LASER embeddings...\n",
      "Loading Spacy models...\n",
      "Warning: Spacy models not found, using NLTK fallback\n",
      "\n",
      "======================================================================\n",
      "Processing 2777 MT pairs\n",
      "Saving intermediate results every 1000 pairs\n",
      "======================================================================\n",
      "\n",
      "Progress: 100/2777 (3.6%) | Elapsed: 0.1min | ETA: 2.4min | Pairs found: 707\n",
      "Progress: 200/2777 (7.2%) | Elapsed: 0.2min | ETA: 2.3min | Pairs found: 1527\n",
      "Progress: 300/2777 (10.8%) | Elapsed: 0.3min | ETA: 2.1min | Pairs found: 2259\n",
      "Progress: 400/2777 (14.4%) | Elapsed: 0.3min | ETA: 2.1min | Pairs found: 3094\n",
      "Progress: 500/2777 (18.0%) | Elapsed: 0.4min | ETA: 2.0min | Pairs found: 4005\n",
      "Progress: 600/2777 (21.6%) | Elapsed: 0.5min | ETA: 2.0min | Pairs found: 4902\n",
      "Progress: 700/2777 (25.2%) | Elapsed: 0.6min | ETA: 1.9min | Pairs found: 5809\n",
      "Progress: 800/2777 (28.8%) | Elapsed: 0.7min | ETA: 1.8min | Pairs found: 6920\n",
      "Progress: 900/2777 (32.4%) | Elapsed: 0.8min | ETA: 1.8min | Pairs found: 7799\n",
      "  Checkpoint saved: 8736 sentence pairs\n",
      "Progress: 1000/2777 (36.0%) | Elapsed: 0.9min | ETA: 1.7min | Pairs found: 8736\n",
      "Progress: 1100/2777 (39.6%) | Elapsed: 1.1min | ETA: 1.6min | Pairs found: 9698\n",
      "Progress: 1200/2777 (43.2%) | Elapsed: 1.2min | ETA: 1.6min | Pairs found: 10813\n",
      "Progress: 1300/2777 (46.8%) | Elapsed: 1.3min | ETA: 1.5min | Pairs found: 12019\n",
      "Progress: 1400/2777 (50.4%) | Elapsed: 1.4min | ETA: 1.4min | Pairs found: 13153\n",
      "Progress: 1500/2777 (54.0%) | Elapsed: 1.6min | ETA: 1.3min | Pairs found: 14549\n",
      "Progress: 1600/2777 (57.6%) | Elapsed: 1.7min | ETA: 1.3min | Pairs found: 15774\n",
      "Progress: 1700/2777 (61.2%) | Elapsed: 1.8min | ETA: 1.2min | Pairs found: 17105\n",
      "Progress: 1800/2777 (64.8%) | Elapsed: 2.0min | ETA: 1.1min | Pairs found: 18539\n",
      "Progress: 1900/2777 (68.4%) | Elapsed: 2.1min | ETA: 1.0min | Pairs found: 19744\n",
      "  Checkpoint saved: 21367 sentence pairs\n",
      "Progress: 2000/2777 (72.0%) | Elapsed: 2.3min | ETA: 0.9min | Pairs found: 21367\n",
      "Progress: 2100/2777 (75.6%) | Elapsed: 2.5min | ETA: 0.8min | Pairs found: 22995\n",
      "Progress: 2200/2777 (79.2%) | Elapsed: 2.6min | ETA: 0.7min | Pairs found: 24456\n",
      "Progress: 2300/2777 (82.8%) | Elapsed: 2.7min | ETA: 0.6min | Pairs found: 25816\n",
      "Progress: 2400/2777 (86.4%) | Elapsed: 2.9min | ETA: 0.4min | Pairs found: 26881\n",
      "Progress: 2500/2777 (90.0%) | Elapsed: 3.0min | ETA: 0.3min | Pairs found: 28302\n",
      "Progress: 2600/2777 (93.6%) | Elapsed: 3.2min | ETA: 0.2min | Pairs found: 29969\n",
      "Progress: 2700/2777 (97.2%) | Elapsed: 3.3min | ETA: 0.1min | Pairs found: 31573\n",
      "  Checkpoint saved: 32581 sentence pairs\n",
      "\n",
      "======================================================================\n",
      "ALIGNMENT RESULTS\n",
      "======================================================================\n",
      "MT paragraph pairs:        2,777\n",
      "Total EN sentences:        44,536\n",
      "Total NO sentences:        46,861\n",
      "Aligned sentence pairs:    32,581\n",
      "Alignment rate:            69.5%\n",
      "Average similarity:        0.890\n",
      "======================================================================\n",
      "\n",
      "Created clean training data: /mnt/d/J/Desktop/language_technology/course/projects_AI/mt_oil/experiments/lora/mt_oli_en_no/data/00_raw_equ/01_processed_equ/clean_equinor_data.json\n",
      "Total sentence pairs: 32,581\n",
      "\n",
      "All done! Check the processed folder for results\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from laserembeddings import Laser\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class MTSentenceAligner:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing LASER embeddings...\")\n",
    "        self.laser = Laser()\n",
    "        \n",
    "        try:\n",
    "            print(\"Loading Spacy models...\")\n",
    "            self.en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "            self.no_nlp = spacy.load(\"nb_core_news_sm\")\n",
    "            print(\"Spacy models loaded successfully\")\n",
    "        except OSError:\n",
    "            print(\"Warning: Spacy models not found, using NLTK fallback\")\n",
    "            import nltk\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            self.en_nlp = None\n",
    "            self.no_nlp = None\n",
    "    \n",
    "    def clean_sentence(self, sentence):\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence.strip())\n",
    "        return sentence\n",
    "    \n",
    "    def count_words(self, text):\n",
    "        return len(text.split())\n",
    "    \n",
    "    def remove_duplicate_title(self, text):\n",
    "        lines = text.strip().split('\\n')\n",
    "        if len(lines) >= 2:\n",
    "            first_line = lines[0].strip()\n",
    "            second_line = lines[1].strip()\n",
    "            if first_line and second_line and first_line in second_line:\n",
    "                return '\\n'.join(lines[1:])\n",
    "        return text\n",
    "    \n",
    "    def split_sentences(self, text, lang='en'):\n",
    "        text = self.remove_duplicate_title(text)\n",
    "        \n",
    "        if lang == 'en' and self.en_nlp:\n",
    "            doc = self.en_nlp(text)\n",
    "            sentences = [sent.text for sent in doc.sents]\n",
    "        elif lang == 'no' and self.no_nlp:\n",
    "            doc = self.no_nlp(text)\n",
    "            sentences = [sent.text for sent in doc.sents]\n",
    "        else:\n",
    "            from nltk.tokenize import sent_tokenize\n",
    "            if lang == 'en':\n",
    "                sentences = sent_tokenize(text)\n",
    "            else:\n",
    "                sentences = sent_tokenize(text, language='norwegian')\n",
    "        \n",
    "        cleaned = []\n",
    "        for s in sentences:\n",
    "            s = self.clean_sentence(s)\n",
    "            if self.count_words(s) >= 3:\n",
    "                cleaned.append(s)\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def merge_short_sentences(self, sentences, min_words=5):\n",
    "        merged = []\n",
    "        buffer = \"\"\n",
    "        \n",
    "        for sent in sentences:\n",
    "            if self.count_words(buffer + \" \" + sent) < min_words:\n",
    "                buffer = (buffer + \" \" + sent).strip()\n",
    "            else:\n",
    "                if buffer:\n",
    "                    merged.append(buffer)\n",
    "                buffer = sent\n",
    "        \n",
    "        if buffer:\n",
    "            merged.append(buffer)\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    def align_sentences(self, en_text, no_text, threshold=0.75):\n",
    "        en_sentences = self.split_sentences(en_text, 'en')\n",
    "        no_sentences = self.split_sentences(no_text, 'no')\n",
    "        \n",
    "        if not en_sentences or not no_sentences:\n",
    "            return []\n",
    "        \n",
    "        en_merged = self.merge_short_sentences(en_sentences)\n",
    "        no_merged = self.merge_short_sentences(no_sentences)\n",
    "        \n",
    "        en_embeddings = self.laser.embed_sentences(en_merged, lang='en')\n",
    "        no_embeddings = self.laser.embed_sentences(no_merged, lang='nb')\n",
    "        \n",
    "        similarity_matrix = cosine_similarity(en_embeddings, no_embeddings)\n",
    "        \n",
    "        aligned_pairs = []\n",
    "        \n",
    "        if abs(len(en_merged) - len(no_merged)) <= 1:\n",
    "            for i in range(min(len(en_merged), len(no_merged))):\n",
    "                similarity = similarity_matrix[i, i] if i < len(no_merged) else 0\n",
    "                if similarity >= threshold:\n",
    "                    aligned_pairs.append({\n",
    "                        'en': en_merged[i],\n",
    "                        'no': no_merged[i] if i < len(no_merged) else \"\",\n",
    "                        'similarity': float(similarity),\n",
    "                        'method': 'sequential'\n",
    "                    })\n",
    "        else:\n",
    "            row_indices, col_indices = linear_sum_assignment(-similarity_matrix)\n",
    "            \n",
    "            for i, j in zip(row_indices, col_indices):\n",
    "                similarity = similarity_matrix[i, j]\n",
    "                if similarity >= threshold:\n",
    "                    aligned_pairs.append({\n",
    "                        'en': en_merged[i],\n",
    "                        'no': no_merged[j],\n",
    "                        'similarity': float(similarity),\n",
    "                        'method': 'hungarian'\n",
    "                    })\n",
    "        \n",
    "        return aligned_pairs\n",
    "    \n",
    "    def process_mt_corpus(self, input_file, output_file, similarity_threshold=0.75, \n",
    "                         batch_save_interval=1000):\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            mt_data = json.load(f)\n",
    "        \n",
    "        all_sentence_pairs = []\n",
    "        doc_stats = []\n",
    "        \n",
    "        total = len(mt_data)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"Processing {total} MT pairs\")\n",
    "        print(f\"Saving intermediate results every {batch_save_interval} pairs\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, item in enumerate(mt_data):\n",
    "            if i % 100 == 0 and i > 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_time = elapsed / i\n",
    "                remaining = (total - i) * avg_time\n",
    "                \n",
    "                progress_pct = i / total * 100\n",
    "                elapsed_min = elapsed / 60\n",
    "                remaining_min = remaining / 60\n",
    "                \n",
    "                print(f\"Progress: {i}/{total} ({progress_pct:.1f}%) | \"\n",
    "                      f\"Elapsed: {elapsed_min:.1f}min | \"\n",
    "                      f\"ETA: {remaining_min:.1f}min | \"\n",
    "                      f\"Pairs found: {len(all_sentence_pairs)}\")\n",
    "            \n",
    "            en_text = item['source']\n",
    "            no_text = item['target']\n",
    "            \n",
    "            aligned_pairs = self.align_sentences(en_text, no_text, similarity_threshold)\n",
    "            \n",
    "            doc_stat = {\n",
    "                'pair_id': i + 1,\n",
    "                'en_sentences': len(self.split_sentences(en_text, 'en')),\n",
    "                'no_sentences': len(self.split_sentences(no_text, 'no')),\n",
    "                'aligned_pairs': len(aligned_pairs),\n",
    "                'avg_similarity': np.mean([p['similarity'] for p in aligned_pairs]) if aligned_pairs else 0\n",
    "            }\n",
    "            doc_stats.append(doc_stat)\n",
    "            \n",
    "            for pair in aligned_pairs:\n",
    "                pair['pair_id'] = i + 1\n",
    "                all_sentence_pairs.append(pair)\n",
    "            \n",
    "            if (i + 1) % batch_save_interval == 0:\n",
    "                self.save_intermediate_results(\n",
    "                    output_file, all_sentence_pairs, doc_stats, \n",
    "                    mt_data, similarity_threshold, i + 1\n",
    "                )\n",
    "        \n",
    "        result = self.save_intermediate_results(\n",
    "            output_file, all_sentence_pairs, doc_stats, \n",
    "            mt_data, similarity_threshold, total\n",
    "        )\n",
    "        \n",
    "        self.print_statistics(doc_stats, all_sentence_pairs, mt_data)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_intermediate_results(self, output_file, sentence_pairs, doc_stats, \n",
    "                                   mt_data, threshold, processed_count):\n",
    "        result = {\n",
    "            'sentence_pairs': sentence_pairs,\n",
    "            'pair_stats': doc_stats,\n",
    "            'total_mt_pairs': len(mt_data),\n",
    "            'processed_pairs': processed_count,\n",
    "            'total_sentence_pairs': len(sentence_pairs),\n",
    "            'similarity_threshold': threshold\n",
    "        }\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"  Checkpoint saved: {len(sentence_pairs)} sentence pairs\")\n",
    "        return result\n",
    "    \n",
    "    def print_statistics(self, doc_stats, sentence_pairs, mt_data):\n",
    "        total_en_sentences = sum(s['en_sentences'] for s in doc_stats)\n",
    "        total_no_sentences = sum(s['no_sentences'] for s in doc_stats)\n",
    "        alignment_rate = len(sentence_pairs) / max(total_en_sentences, total_no_sentences) * 100\n",
    "        avg_similarity = np.mean([p['similarity'] for p in sentence_pairs])\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ALIGNMENT RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"MT paragraph pairs:        {len(mt_data):,}\")\n",
    "        print(f\"Total EN sentences:        {total_en_sentences:,}\")\n",
    "        print(f\"Total NO sentences:        {total_no_sentences:,}\")\n",
    "        print(f\"Aligned sentence pairs:    {len(sentence_pairs):,}\")\n",
    "        print(f\"Alignment rate:            {alignment_rate:.1f}%\")\n",
    "        print(f\"Average similarity:        {avg_similarity:.3f}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    def create_clean_training_data(self, aligned_file, output_file):\n",
    "        with open(aligned_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        clean_pairs = []\n",
    "        for pair in data['sentence_pairs']:\n",
    "            clean_pairs.append({\n",
    "                'source': pair['en'],\n",
    "                'target': pair['no'],\n",
    "                'source_lang': 'en',\n",
    "                'target_lang': 'no'\n",
    "            })\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(clean_pairs, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Created clean training data: {output_file}\")\n",
    "        print(f\"Total sentence pairs: {len(clean_pairs):,}\")\n",
    "\n",
    "\n",
    "def quick_test(input_file, n_samples=3):\n",
    "    aligner = MTSentenceAligner()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"QUICK TEST: Processing first {n_samples} paragraph pairs\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    for i, item in enumerate(data[:n_samples]):\n",
    "        print(f\"\\nParagraph Pair {i+1}\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"EN: {item['source'][:100]}...\")\n",
    "        print(f\"NO: {item['target'][:100]}...\")\n",
    "        \n",
    "        aligned = aligner.align_sentences(item['source'], item['target'])\n",
    "        print(f\"\\nFound {len(aligned)} sentence pairs:\\n\")\n",
    "        \n",
    "        for j, pair in enumerate(aligned, 1):\n",
    "            print(f\"  Pair {j} | Similarity: {pair['similarity']:.3f} | Method: {pair['method']}\")\n",
    "            print(f\"    EN: {pair['en'][:80]}...\")\n",
    "            print(f\"    NO: {pair['no'][:80]}...\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    DATA_DIR = Path(\"/mnt/d/J/Desktop/language_technology/course/projects_AI/mt_oil/experiments/lora/mt_oli_en_no/data/00_raw_equ\")\n",
    "    INPUT_FILE = DATA_DIR / \"equinor_data.json\"\n",
    "    OUTPUT_DIR = DATA_DIR / \"01_processed_equ\"\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    MODE = \"full\" \n",
    "    \n",
    "    if MODE == \"test\":\n",
    "        print(\"\\nRUNNING IN TEST MODE\")\n",
    "        quick_test(INPUT_FILE, n_samples=5)\n",
    "        \n",
    "    elif MODE == \"full\":\n",
    "        print(\"\\nRUNNING IN FULL MODE\")\n",
    "        aligner = MTSentenceAligner()\n",
    "        \n",
    "        result = aligner.process_mt_corpus(\n",
    "            input_file=INPUT_FILE,\n",
    "            output_file=OUTPUT_DIR / \"equinor_aligned_full.json\",\n",
    "            similarity_threshold=0.75,\n",
    "            batch_save_interval=1000\n",
    "        )\n",
    "        \n",
    "        aligner.create_clean_training_data(\n",
    "            aligned_file=OUTPUT_DIR / \"equinor_aligned_full.json\",\n",
    "            output_file=OUTPUT_DIR / \"clean_equinor_data.json\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\nAll done! Check the processed folder for results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b00a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
